{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL.Image\n",
    "from collections import deque\n",
    "from pyvirtualdisplay import Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a virtual display to render the Lunar Lander environment.\n",
    "Display(visible=0, size=(840, 480)).start();\n",
    "env = gym.make('LunarLander-v2', render_mode=\"rgb_array\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=600x400>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()\n",
    "PIL.Image.fromarray(env.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Shape: (8,)\n",
      "Number of actions: 4\n",
      "Initial State: (array([0.007, 1.405, 0.731, -0.257, -0.008, -0.166, 0.000, 0.000],\n",
      "      dtype=float32), {})\n",
      "Action: 0\n",
      "Next State: [0.014 1.399 0.730 -0.283 -0.017 -0.164 0.000 0.000]\n",
      "Reward Received: -0.9844890838578806\n",
      "Episode Terminated: False\n",
      "Info: {}\n",
      "Episode 2 | Total point average of the last 100 episodes: -251.71"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arindams_mac_m2_pro/opt/anaconda3/envs/py310/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n",
      "/Users/arindams_mac_m2_pro/opt/anaconda3/envs/py310/lib/python3.10/site-packages/torch/optim/optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100 | Total point average of the last 100 episodes: -165.40\n",
      "Episode 200 | Total point average of the last 100 episodes: -68.807\n",
      "Episode 300 | Total point average of the last 100 episodes: -24.07\n",
      "Episode 400 | Total point average of the last 100 episodes: 28.779\n",
      "Episode 500 | Total point average of the last 100 episodes: 78.58\n",
      "Episode 600 | Total point average of the last 100 episodes: 63.61\n",
      "Episode 700 | Total point average of the last 100 episodes: 111.00\n",
      "Episode 800 | Total point average of the last 100 episodes: 159.97\n",
      "Episode 900 | Total point average of the last 100 episodes: 25.266\n",
      "Episode 1000 | Total point average of the last 100 episodes: 163.02\n",
      "Episode 1100 | Total point average of the last 100 episodes: 173.69\n",
      "Episode 1200 | Total point average of the last 100 episodes: 161.00\n",
      "Episode 1290 | Total point average of the last 100 episodes: 200.15\n",
      "\n",
      "Environment solved in 1290 episodes with total reward -5.823218000669525!\n"
     ]
    }
   ],
   "source": [
    "state_size = env.observation_space.shape\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "print('State Shape:', state_size)\n",
    "print('Number of actions:', num_actions)\n",
    "# Reset the environment and get the initial state.\n",
    "initial_state = env.reset()\n",
    "# Select an action\n",
    "action = 0\n",
    "\n",
    "# Run a single time step of the environment's dynamics with the given action.\n",
    "next_state, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "with np.printoptions(formatter={'float': '{:.3f}'.format}):\n",
    "    print(\"Initial State:\", initial_state)\n",
    "    print(\"Action:\", action)\n",
    "    print(\"Next State:\", next_state)\n",
    "    print(\"Reward Received:\", reward)\n",
    "    print(\"Episode Terminated:\", done)\n",
    "    print(\"Info:\", info)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.out = nn.Linear(128, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "# Hyperparameters\n",
    "# Hyperparameters\n",
    "GAMMA = 0.99\n",
    "EPS_START = 1.0\n",
    "EPS_END = 0.01\n",
    "EPS_DECAY = 500\n",
    "TARGET_UPDATE = 10\n",
    "BATCH_SIZE = 64\n",
    "LR = 0.001\n",
    "MEMORY_SIZE = 10000\n",
    "NUM_STEPS_FOR_UPDATE = 5\n",
    "\n",
    "# Epsilon decay function\n",
    "def get_new_eps(epsilon):\n",
    "    return max(EPS_END, epsilon * 0.995)\n",
    "\n",
    "# Memory buffer\n",
    "memory_buffer = deque(maxlen=MEMORY_SIZE)\n",
    "\n",
    "q_network = DQN(state_size[0], num_actions)\n",
    "target_q_network = DQN(state_size[0], num_actions)\n",
    "\n",
    "target_q_network.load_state_dict(q_network.state_dict())\n",
    "optimizer = optim.AdamW(q_network.parameters(), lr=LR)\n",
    "\n",
    "# Epsilon greedy approach to take action\n",
    "def select_action(state, steps_done):\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * np.exp(-1 * steps_done/ EPS_DECAY)\n",
    "    if np.random.rand() < eps_threshold:\n",
    "        return torch.tensor(np.array([np.random.randint(0, num_actions)]).reshape(1, 1), dtype=torch.long)\n",
    "    with torch.no_grad():\n",
    "        return q_network(state).max(1)[1].view(1, 1)\n",
    "\n",
    "# model optmization strategy\n",
    "def optimize_model(replay_memory):\n",
    "    if len(replay_memory) < BATCH_SIZE:\n",
    "        return\n",
    "    batch = random.sample(replay_memory, BATCH_SIZE)\n",
    "    states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "    states = torch.cat(states)\n",
    "    actions = torch.cat(actions)\n",
    "    rewards = torch.tensor(rewards, dtype=torch.float32).unsqueeze(1)\n",
    "    next_states = torch.cat(next_states)\n",
    "    dones = torch.tensor(dones, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "    Q_values = q_network(states).gather(1, actions)\n",
    "    next_Q_values = target_q_network(next_states).max(1)[0].unsqueeze(1)\n",
    "    target = rewards + (1 - dones) * GAMMA * next_Q_values\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    loss = criterion(Q_values, target)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "epsilon = EPS_START\n",
    "NUM_EPISODES = 2000\n",
    "MAX_ITERATIONS = 1000\n",
    "NUM_P_AV = 100 \n",
    "\n",
    "episode_rewards = []\n",
    "\n",
    "for episode in range(NUM_EPISODES):\n",
    "    state, _ = env.reset()\n",
    "    state = torch.tensor([state], dtype=torch.float32)\n",
    "    total_reward = 0\n",
    "    steps_done = 0\n",
    "\n",
    "    for _ in range(MAX_ITERATIONS):\n",
    "        # Select action using epsilon-greedy policy\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = torch.tensor([[np.random.randint(num_actions)]], dtype=torch.long)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                action = q_network(state).max(1)[1].view(1, 1)\n",
    "\n",
    "        next_state, reward, done, _, _= env.step(action.item())\n",
    "        next_state = torch.tensor([next_state], dtype=torch.float32)\n",
    "\n",
    "        # Store experience in memory buffer\n",
    "        memory_buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "        # Update the network every NUM_STEPS_FOR_UPDATE time steps\n",
    "        if steps_done % NUM_STEPS_FOR_UPDATE == 0 and len(memory_buffer) >= BATCH_SIZE:\n",
    "            experiences = random.sample(memory_buffer, BATCH_SIZE)\n",
    "            optimize_model(experiences)\n",
    "\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        steps_done += 1\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Update epsilon\n",
    "    epsilon = get_new_eps(epsilon)\n",
    "\n",
    "    # Update target network\n",
    "    if episode % TARGET_UPDATE == 0:\n",
    "        target_q_network.load_state_dict(q_network.state_dict())\n",
    "\n",
    "    episode_rewards.append(total_reward)\n",
    "    av_latest_points = np.mean(episode_rewards[-NUM_P_AV:])\n",
    "    print(f\"\\rEpisode {episode+1} | Total point average of the last {NUM_P_AV} episodes: {av_latest_points:.2f}\", end=\"\")\n",
    "\n",
    "    if (episode+1) % NUM_P_AV == 0:\n",
    "        print(f\"\\rEpisode {episode+1} | Total point average of the last {NUM_P_AV} episodes: {av_latest_points:.2f}\")\n",
    "\n",
    "    # If the total reward for the episode exceeds 200, stop the training and save the weights.\n",
    "    if av_latest_points > 200:\n",
    "        print(f\"\\n\\nEnvironment solved in {episode+1} episodes with total reward {total_reward}!\")\n",
    "        torch.save(q_network.state_dict(), 'final_weights.pth')\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import base64\n",
    "import IPython\n",
    "\n",
    "def embed_mp4(filename):\n",
    "    \"\"\"Embeds an mp4 file in the notebook.\"\"\"\n",
    "    video = open(filename,'rb').read()\n",
    "    b64 = base64.b64encode(video)\n",
    "    tag = '''\n",
    "    <video width=\"840\" height=\"480\" controls>\n",
    "    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\">\n",
    "    Your browser does not support the video tag.\n",
    "    </video>'''.format(b64.decode())\n",
    "    return IPython.display.HTML(tag)\n",
    "        \n",
    "        \n",
    "def create_video(filename, env, q_network, fps=30):\n",
    "    with imageio.get_writer(filename, fps=fps) as video:\n",
    "        done = False\n",
    "        state, _ = env.reset()\n",
    "        frame = env.render()\n",
    "        video.append_data(frame)\n",
    "        while not done:    \n",
    "            state = torch.tensor([state], dtype=torch.float32)\n",
    "            with torch.no_grad():\n",
    "                action = q_network(state).max(1)[1].view(1, 1)\n",
    "            state, _, done, _, _ = env.step(action.item())\n",
    "            frame = env.render()\n",
    "            video.append_data(frame)\n",
    "\n",
    "env = gym.make('LunarLander-v2', render_mode=\"rgb_array\")\n",
    "create_video(\"./video/lunar_lander.mp4\", env, q_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"./video/lunar_lander.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Video\n",
    "Video(\"./video/lunar_lander.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
